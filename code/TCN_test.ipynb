{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5723c3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tcn in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-tcn) (2.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-tcn) (2.3.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->pytorch-tcn) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->pytorch-tcn) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->pytorch-tcn) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->pytorch-tcn) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->pytorch-tcn) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->pytorch-tcn) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch->pytorch-tcn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oldys\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->pytorch-tcn) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-tcn torchvision pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c450d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (updated) parameters: 110339\n"
     ]
    }
   ],
   "source": [
    "# Updated dataset for annotation CSV format (each split folder contains _annotations.csv + images)\n",
    "import os\n",
    "import csv\n",
    "from typing import List, Tuple, Dict\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters (adjust as needed)\n",
    "NUM_CLASSES = 3  # Rock, Paper, Scissors\n",
    "SEQ_LEN = 8      # number of early frames to use / pseudo sequence length\n",
    "FRAME_SIZE = 128 # resize for faster encoding\n",
    "FEATURE_DIM = 64 # per-frame embedding size (becomes num_inputs for TCN)\n",
    "CHANNELS = [64, 64]  # TCN hidden channels\n",
    "KERNEL_SIZE = 3\n",
    "DROPOUT = 0.1\n",
    "EARLY_WEIGHT_MODE = 'linear'  # 'linear' or 'exp'\n",
    "ALPHA_EXP = 2.0  # used if EARLY_WEIGHT_MODE == 'exp'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Class names as they appear in the CSV\n",
    "CLASS_MAP = {'Rock':0,'Paper':1,'Scissors':2}\n",
    "\n",
    "# Augmentations; different random transform per repeated frame to simulate temporal evolution\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.05),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "noaug_transform = transforms.Compose([\n",
    "    transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class AnnotatedGestureDataset(Dataset):\n",
    "    \"\"\"Dataset reading Roboflow-style _annotations.csv with columns:\n",
    "    filename,width,height,class,xmin,ymin,xmax,ymax\n",
    "    If multiple boxes per image, picks the largest area.\n",
    "    Returns pseudo sequence by repeating cropped region with augmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, seq_len: int = SEQ_LEN, augment: bool = True, annotations_file: str = '_annotations.csv'):\n",
    "        self.root = root\n",
    "        self.seq_len = seq_len\n",
    "        self.augment = augment\n",
    "        self.annotations_path = os.path.join(root, annotations_file)\n",
    "        self.transform = base_transform if augment else noaug_transform\n",
    "        self.samples: List[Tuple[str,int,Tuple[int,int,int,int]]] = []  # (image_path,label,(xmin,ymin,xmax,ymax))\n",
    "        if not os.path.isfile(self.annotations_path):\n",
    "            raise FileNotFoundError(f'Annotations file not found: {self.annotations_path}')\n",
    "        # aggregate boxes per filename\n",
    "        boxes_per_file: Dict[str, List[Tuple[int,int,int,int,int]]] = {}\n",
    "        with open(self.annotations_path, 'r', newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                fname = row['filename']\n",
    "                cls = row['class']\n",
    "                if cls not in CLASS_MAP:\n",
    "                    continue\n",
    "                xmin = int(float(row['xmin']))\n",
    "                ymin = int(float(row['ymin']))\n",
    "                xmax = int(float(row['xmax']))\n",
    "                ymax = int(float(row['ymax']))\n",
    "                area = (xmax - xmin) * (ymax - ymin)\n",
    "                boxes_per_file.setdefault(fname, []).append((area, xmin, ymin, xmax, ymax, CLASS_MAP[cls]))\n",
    "        for fname, box_list in boxes_per_file.items():\n",
    "            # choose largest box\n",
    "            box_list.sort(key=lambda x: x[0], reverse=True)\n",
    "            _, xmin, ymin, xmax, ymax, label = box_list[0]\n",
    "            img_path = os.path.join(root, fname)\n",
    "            if os.path.isfile(img_path):\n",
    "                self.samples.append((img_path, label, (xmin, ymin, xmax, ymax)))\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f'No samples parsed from {self.annotations_path}. Check class names or paths.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path, label, (xmin, ymin, xmax, ymax) = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        # clamp box\n",
    "        xmin = max(0, xmin); ymin = max(0, ymin)\n",
    "        xmax = min(img.width, xmax); ymax = min(img.height, ymax)\n",
    "        crop = img.crop((xmin, ymin, xmax, ymax))\n",
    "        frames = []\n",
    "        for t in range(self.seq_len):\n",
    "            frames.append(self.transform(crop))\n",
    "        seq = torch.stack(frames, dim=0)  # (T,C,H,W)\n",
    "        return seq, label\n",
    "\n",
    "class FrameEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=FEATURE_DIM):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(64, out_dim)\n",
    "    def forward(self, x):\n",
    "        z = self.net(x).view(x.size(0), -1)\n",
    "        return self.proj(z)\n",
    "\n",
    "class EarlyGestureTCN(nn.Module):\n",
    "    def __init__(self, feature_dim=FEATURE_DIM, channels=CHANNELS, kernel_size=KERNEL_SIZE, dropout=DROPOUT, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.encoder = FrameEncoder(feature_dim)\n",
    "        from pytorch_tcn import TCN as LibTCN\n",
    "        self.tcn = LibTCN(\n",
    "            num_inputs=feature_dim,\n",
    "            num_channels=channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            causal=True,\n",
    "            use_norm='weight_norm',\n",
    "            activation='relu',\n",
    "            input_shape='NCL'\n",
    "        )\n",
    "        self.classifier = nn.Linear(channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, seq_frames):\n",
    "        B, T, C, H, W = seq_frames.shape\n",
    "        frames = seq_frames.view(B*T, C, H, W)\n",
    "        feats = self.encoder(frames)            # (B*T,F)\n",
    "        feats = feats.view(B, T, -1).transpose(1,2)  # (B,F,T)\n",
    "        tcn_out = self.tcn(feats)               # (B,F_last,T)\n",
    "        logits_time = self.classifier(tcn_out.transpose(1,2))  # (B,T,num_classes)\n",
    "        return logits_time\n",
    "\n",
    "    def predict_early(self, seq_frames, thresh: float = 0.6):\n",
    "        with torch.no_grad():\n",
    "            logits_time = self.forward(seq_frames)\n",
    "            probs_time = torch.softmax(logits_time, dim=-1)\n",
    "            preds = []\n",
    "            for b in range(probs_time.size(0)):\n",
    "                earliest = None\n",
    "                for t in range(probs_time.size(1)):\n",
    "                    p = probs_time[b,t]\n",
    "                    if p.max().item() >= thresh:\n",
    "                        earliest = (t, p.argmax().item(), p.max().item())\n",
    "                        break\n",
    "                if earliest is None:\n",
    "                    p = probs_time[b,-1]\n",
    "                    earliest = (probs_time.size(1)-1, p.argmax().item(), p.max().item())\n",
    "                preds.append(earliest)\n",
    "        return preds, probs_time\n",
    "\n",
    "# Time weighting for early emphasis\n",
    "def time_weights(T: int, mode: str = EARLY_WEIGHT_MODE, alpha: float = ALPHA_EXP, device=DEVICE):\n",
    "    if mode == 'linear':\n",
    "        w = torch.linspace(1.0, 0.3, steps=T)\n",
    "    elif mode == 'exp':\n",
    "        t = torch.arange(T)\n",
    "        w = torch.exp(-alpha * t / T)\n",
    "    else:\n",
    "        w = torch.ones(T)\n",
    "    w = w / w.sum() * T\n",
    "    return w.to(device)\n",
    "\n",
    "# Early loss function\n",
    "def compute_loss(logits_time, labels):\n",
    "    B,T,C = logits_time.shape\n",
    "    weights = time_weights(T)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    labels_time = labels.unsqueeze(1).expand(B,T).reshape(B*T)\n",
    "    logits_flat = logits_time.view(B*T, C)\n",
    "    per_frame_loss = loss_fn(logits_flat, labels_time).view(B,T)\n",
    "    weighted = per_frame_loss * weights\n",
    "    return weighted.mean(), per_frame_loss.mean(dim=0).detach(), weights.detach()\n",
    "\n",
    "# Instantiate model (re-instantiated if cell re-run)\n",
    "model_early = EarlyGestureTCN().to(DEVICE)\n",
    "print('Model (updated) parameters:', sum(p.numel() for p in model_early.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a389d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3939 Val samples: 338\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m logits_time = model_early(seq)\n\u001b[32m     31\u001b[39m loss, _, _ = compute_loss(logits_time, label)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m optimizer.step()\n\u001b[32m     34\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oldys\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oldys\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oldys\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop adapted for annotation CSV dataset\n",
    "import math, time\n",
    "from torch.optim import Adam\n",
    "\n",
    "DATA_ROOT = os.path.join(os.getcwd(), 'Rock Paper Scissors SXSW.v14i.tensorflow', 'train')\n",
    "VAL_ROOT = os.path.join(os.getcwd(), 'Rock Paper Scissors SXSW.v14i.tensorflow', 'valid')\n",
    "\n",
    "train_ds = AnnotatedGestureDataset(DATA_ROOT, seq_len=SEQ_LEN, augment=True)\n",
    "val_ds   = AnnotatedGestureDataset(VAL_ROOT,   seq_len=SEQ_LEN, augment=False)\n",
    "print('Train samples:', len(train_ds), 'Val samples:', len(val_ds))\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3\n",
    "EPOCHS = 5\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "optimizer = Adam(model_early.parameters(), lr=LR)\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model_early.train()\n",
    "    total_loss = 0.0\n",
    "    batches = 0\n",
    "    for seq, label in train_loader:\n",
    "        seq = seq.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits_time = model_early(seq)\n",
    "        loss, _, _ = compute_loss(logits_time, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "    avg_loss = total_loss / max(1,batches)\n",
    "\n",
    "    # Validation metrics including early coverage and average first correct timestep\n",
    "    model_early.eval()\n",
    "    correct_final = 0\n",
    "    correct_early = 0\n",
    "    total = 0\n",
    "    time_to_first_correct = []\n",
    "    with torch.no_grad():\n",
    "        for seq, label in val_loader:\n",
    "            seq = seq.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            logits_time = model_early(seq)\n",
    "            probs_time = torch.softmax(logits_time, dim=-1)  # (B,T,C)\n",
    "            final_preds = probs_time[:,-1].argmax(dim=-1)\n",
    "            correct_final += (final_preds == label).sum().item()\n",
    "            B = seq.size(0)\n",
    "            for b in range(B):\n",
    "                gt = label[b].item()\n",
    "                earliest = None\n",
    "                for t in range(probs_time.size(1)):\n",
    "                    pred_t = probs_time[b,t].argmax().item()\n",
    "                    if pred_t == gt:\n",
    "                        earliest = t\n",
    "                        break\n",
    "                if earliest is not None:\n",
    "                    correct_early += 1\n",
    "                    time_to_first_correct.append(earliest+1)\n",
    "            total += B\n",
    "    final_acc = correct_final / max(1,total)\n",
    "    early_cov = correct_early / max(1,total)\n",
    "    avg_time_first = sum(time_to_first_correct)/len(time_to_first_correct) if time_to_first_correct else math.nan\n",
    "\n",
    "    print(f\"Epoch {epoch}: train_loss={avg_loss:.4f} val_final_acc={final_acc:.3f} early_cover={early_cov:.3f} avg_first_correct_t={avg_time_first}\")\n",
    "\n",
    "    if final_acc > best_val_acc:\n",
    "        best_val_acc = final_acc\n",
    "        torch.save({'model':model_early.state_dict()}, 'best_early_tcn.pt')\n",
    "        print('Saved new best model.')\n",
    "\n",
    "print('Training complete. Best final acc:', best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2828d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming / real-time inference utilities\n",
    "import collections\n",
    "from torchvision import transforms as T\n",
    "\n",
    "stream_transform = T.Compose([\n",
    "    T.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "class StreamingEarlyPredictor:\n",
    "    def __init__(self, model: EarlyGestureTCN, seq_len: int = SEQ_LEN, threshold: float = 0.6):\n",
    "        self.model = model\n",
    "        self.seq_len = seq_len\n",
    "        self.threshold = threshold\n",
    "        self.buffer = collections.deque(maxlen=seq_len)\n",
    "        self.model.eval()\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer.clear()\n",
    "        # If using library TCN internal buffers for causal conv, we can also call:\n",
    "        self.model.tcn.reset_buffers()\n",
    "\n",
    "    def update(self, frame_pil: Image.Image):\n",
    "        tensor = stream_transform(frame_pil).unsqueeze(0)  # (1,C,H,W)\n",
    "        self.buffer.append(tensor)\n",
    "        if len(self.buffer) < 2:\n",
    "            return None  # need at least 2 frames maybe\n",
    "        # Build sequence (1,T,C,H,W)\n",
    "        seq = torch.cat(list(self.buffer), dim=0).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits_time = self.model(seq)\n",
    "            probs_time = torch.softmax(logits_time, dim=-1)[0]  # (T,C)\n",
    "        # earliest confident\n",
    "        for t in range(probs_time.size(0)):\n",
    "            p = probs_time[t]\n",
    "            if p.max().item() >= self.threshold:\n",
    "                return {'timestep': t+1, 'pred': int(p.argmax().item()), 'confidence': float(p.max().item())}\n",
    "        return None\n",
    "\n",
    "# Example usage (pseudo):\n",
    "# predictor = StreamingEarlyPredictor(model_early)\n",
    "# for frame in webcam_frames():\n",
    "#     result = predictor.update(frame)\n",
    "#     if result:\n",
    "#         print('EARLY PRED', result)\n",
    "#         # optionally reset after a stable prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras TCN alternative (only if you later switch to TensorFlow).\n",
    "# # NOTE: This is illustrative; run in a TF environment. Here we just store the code.\n",
    "# keras_tcn_code = r\"\"\"\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# NUM_CLASSES = 3\n",
    "# SEQ_LEN = 8\n",
    "# FEATURE_DIM = 64  # if you extract per-frame CNN features separately\n",
    "\n",
    "# # Frame encoder (simple) - assuming input (H,W,3)\n",
    "# def build_frame_encoder():\n",
    "#     inputs = keras.Input(shape=(128,128,3))\n",
    "#     x = layers.Conv2D(32,3,strides=2,padding='same',activation='relu')(inputs)\n",
    "#     x = layers.Conv2D(64,3,strides=2,padding='same',activation='relu')(x)\n",
    "#     x = layers.Conv2D(64,3,strides=2,padding='same',activation='relu')(x)\n",
    "#     x = layers.GlobalAveragePooling2D()(x)\n",
    "#     outputs = layers.Dense(FEATURE_DIM)(x)\n",
    "#     return keras.Model(inputs, outputs, name='frame_encoder')\n",
    "\n",
    "# frame_encoder = build_frame_encoder()\n",
    "\n",
    "# # TCN block (simplified causal dilated conv stack)\n",
    "# def tcn_block(x, filters, kernel_size, dilation_rate, dropout):\n",
    "#     prev = x\n",
    "#     x = layers.Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
    "#     x = layers.Dropout(dropout)(x)\n",
    "#     x = layers.Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
    "#     if prev.shape[-1] != filters:\n",
    "#         prev = layers.Conv1D(filters, 1, padding='same')(prev)\n",
    "#     return layers.Add()([prev, x])\n",
    "\n",
    "# # Full model: input (T, H, W, 3)\n",
    "# video_inputs = keras.Input(shape=(SEQ_LEN,128,128,3))\n",
    "# # TimeDistributed encoding\n",
    "# encoded = layers.TimeDistributed(frame_encoder)(video_inputs)  # (B,T,F)\n",
    "# # TCN stacks\n",
    "# x = encoded\n",
    "# for d in [1,2,4]:\n",
    "#     x = tcn_block(x, 64, 3, dilation_rate=d, dropout=0.1)\n",
    "# # Per-timestep classification\n",
    "# logits = layers.TimeDistributed(layers.Dense(NUM_CLASSES))(x)  # (B,T,C)\n",
    "# # Optionally final timestep output\n",
    "# final_logits = layers.Lambda(lambda t: t[:,-1])(logits)\n",
    "# model = keras.Model(video_inputs, [logits, final_logits])\n",
    "# model.compile(optimizer='adam', loss=[keras.losses.CategoricalCrossentropy(from_logits=True), keras.losses.CategoricalCrossentropy(from_logits=True)], loss_weights=[1.0, 0.3])\n",
    "# print(model.summary())\n",
    "# \"\"\"\n",
    "# print('Stored Keras TCN example in variable keras_tcn_code (string).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
