{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2666e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & collection plan for early gesture prediction (Rock/Paper/Scissors)\n",
    "# ----------------------------------------------------------\n",
    "# GOAL: Predict gesture class ASAP before completion.\n",
    "# KEY IDEA: Train on progressive prefixes of gesture videos so model learns\n",
    "# to classify from partial motion context, not only full final pose.\n",
    "#\n",
    "# 1. RECORDING PROTOCOL\n",
    "#    - Camera: laptop/webcam, frontal view, consistent background if possible.\n",
    "#    - Resolution: 640x480 or 720p (higher adds cost; downsample later).\n",
    "#    - FPS: 15–30 (choose 20 for balance).\n",
    "#    - Clip length: capture from neutral hand start -> final pose hold (~1.0–1.5s).\n",
    "#      Total frames per clip ≈ 20–30 at 20 FPS.\n",
    "#    - Classes: rock, paper, scissors (folder/class-based or filename-based labels).\n",
    "#    - Variations: different users, lighting, hand orientations (side, frontal),\n",
    "#      distances (near/far), left/right hand.\n",
    "#\n",
    "# 2. DATA VOLUME (minimum viable):\n",
    "#    - Per class: 300–500 clips for baseline; scaling to 1000+ improves robustness.\n",
    "#    - Total frames processed after prefix expansion rises (see below).\n",
    "#\n",
    "# 3. STORAGE / VM TRANSFER CONCERNS\n",
    "#    - Raw video size grows quickly; prefer:\n",
    "#        a. Compress: H.264 MP4, ~1–2 Mbps.\n",
    "#        b. Downscale resolution early (e.g. ffmpeg scale=320:240 if quality sufficient).\n",
    "#        c. Extract frames on HOST, transfer frames (.jpg) instead of video if VM IO limited.\n",
    "#        d. Optionally store only first N frames (e.g. first 24) for each clip.\n",
    "#    - Approx size estimate:\n",
    "#        500 clips/class * 3 classes = 1500 clips.\n",
    "#        Each clip 1.5s at 20 FPS -> 30 frames -> 1500*30 = 45k frames.\n",
    "#        JPEG 20–40KB each ≈ 0.9–1.8GB worst-case (optimize with lower quality).\n",
    "#\n",
    "# 4. TRAINING STRATEGY FOR EARLY PREDICTION\n",
    "#    - From each full clip produce multiple PREFIX sequences length T where\n",
    "#      T in {4,6,8,10,...,MAX_FRAMES}. (Curriculum / multi-prefix sampling.)\n",
    "#    - Weight earlier timesteps heavier (already implemented time_weights).\n",
    "#    - OPTION: Binary “decision made” flag—stop predicting earlier once confidence threshold crossed.\n",
    "#\n",
    "# 5. FEED FULL VIDEOS OR CUT?\n",
    "#    - For early classification we do NOT need entire finished gesture every time.\n",
    "#    - We SHOULD keep the full clip to generate prefixes dynamically.\n",
    "#    - During training sample a random prefix length L (e.g. uniform 4..SEQ_LEN_TARGET),\n",
    "#      pad if needed, pass to model; model outputs per timestep logits.\n",
    "#\n",
    "# 6. LABELS FOR PREFIXES\n",
    "#    - Entire prefix labeled with final gesture class (static label).\n",
    "#    - Optional: add a \"none\" class for frames before motion begins (requires start frame annotation).\n",
    "#    - If start-of-motion annotation unavailable, assume clip trimmed to begin near motion onset.\n",
    "#\n",
    "# 7. IMPLEMENTATION OUTLINE:\n",
    "#    - VideoEarlyGestureDataset: loads video, extracts frames, caches optionally.\n",
    "#    - Returns a sequence tensor (T,C,H,W) with (T <= MAX_SEQ_LEN).\n",
    "#    - On __getitem__, randomly selects prefix length L >= MIN_PREFIX.\n",
    "#\n",
    "# 8. OPTIONAL OPTIMIZATION:\n",
    "#    - Pre-extract frames to disk: dataset_root/class_name/clip_id/frame_%03d.jpg\n",
    "#    - Speeds up I/O vs decoding every epoch.\n",
    "#\n",
    "# 9. REAL-TIME INFERENCE:\n",
    "#    - Maintain a sliding buffer of last K frames.\n",
    "#    - After each new frame, run model, check confidence threshold.\n",
    "#    - Reset buffer when prediction stabilized or after fixed timeout.\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CODE: Utilities for frame extraction (offline), dataset class, and loader.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import cv2\n",
    "from typing import List, Tuple, Optional\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Reuse existing constants or redefine if running standalone\n",
    "NUM_CLASSES = 3\n",
    "CLASS_MAP = {'rock':0, 'paper':1, 'scissors':2}\n",
    "INV_CLASS_MAP = {v:k for k,v in CLASS_MAP.items()}\n",
    "\n",
    "FRAME_SIZE = 128\n",
    "MAX_SEQ_LEN = 16          # Upper bound of frames fed to model (increase if gesture longer)\n",
    "MIN_PREFIX_LEN = 4        # Smallest prefix used in training\n",
    "AUGMENT = True            # Enable per-frame augmentation for robustness\n",
    "CACHE_FRAMES = True       # If True, pre-extract frames to folders\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transforms\n",
    "base_video_transform = transforms.Compose([\n",
    "    transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "    transforms.ColorJitter(0.25,0.25,0.25,0.05),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "eval_video_transform = transforms.Compose([\n",
    "    transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def infer_label_from_filename(name: str) -> Optional[int]:\n",
    "    low = name.lower()\n",
    "    for k in CLASS_MAP.keys():\n",
    "        if k in low:\n",
    "            return CLASS_MAP[k]\n",
    "    return None\n",
    "\n",
    "def extract_frames_from_video(path: str, max_frames: int = MAX_SEQ_LEN, every_n: int = 1) -> List[Image.Image]:\n",
    "    \"\"\"Decode video and return up to max_frames PIL RGB frames subsampled by every_n.\"\"\"\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        return []\n",
    "    frames = []\n",
    "    frame_idx = 0\n",
    "    while len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % every_n == 0:\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(rgb))\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def preextract_video_frames(video_root: str, output_root: str, pattern: str = \"*.mp4\"):\n",
    "    \"\"\"Pre-extract frames of each video to output_root/class/clip_basename/frame_###.jpg.\"\"\"\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    videos = glob.glob(os.path.join(video_root, pattern))\n",
    "    for vp in videos:\n",
    "        label = infer_label_from_filename(os.path.basename(vp))\n",
    "        if label is None:\n",
    "            continue\n",
    "        cls_name = INV_CLASS_MAP[label]\n",
    "        out_dir = os.path.join(output_root, cls_name, os.path.splitext(os.path.basename(vp))[0])\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        if len(glob.glob(os.path.join(out_dir, \"frame_*.jpg\"))) > 0:\n",
    "            continue  # already extracted\n",
    "        frames = extract_frames_from_video(vp, max_frames=MAX_SEQ_LEN*2)  # extract extra for prefix variation\n",
    "        if not frames:\n",
    "            continue\n",
    "        for i, img in enumerate(frames):\n",
    "            img.save(os.path.join(out_dir, f\"frame_{i:03d}.jpg\"))\n",
    "\n",
    "class VideoEarlyGestureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads video clips for early gesture classification.\n",
    "    Supports on-the-fly decoding OR cached frames directory structure:\n",
    "        root/\n",
    "          rock/\n",
    "            clip1.mp4\n",
    "            clip2.mp4\n",
    "          paper/\n",
    "            ...\n",
    "    OR if CACHE_FRAMES:\n",
    "        root_frames/\n",
    "          rock/clip1/frame_000.jpg ...\n",
    "    Returns (seq_tensor, label).\n",
    "    Sequence is a random prefix length between MIN_PREFIX_LEN and MAX_SEQ_LEN (training mode).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 video_root: str,\n",
    "                 max_seq_len: int = MAX_SEQ_LEN,\n",
    "                 min_prefix_len: int = MIN_PREFIX_LEN,\n",
    "                 cache_frames_root: Optional[str] = None,\n",
    "                 training: bool = True,\n",
    "                 augment: bool = AUGMENT):\n",
    "        self.video_root = video_root\n",
    "        self.cache_root = cache_frames_root\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.min_prefix_len = min_prefix_len\n",
    "        self.training = training\n",
    "        self.transform = base_video_transform if (training and augment) else eval_video_transform\n",
    "        self.samples: List[Tuple[str,int,bool]] = []  # (path,label,is_cached)\n",
    "        if cache_frames_root and os.path.isdir(cache_frames_root):\n",
    "            # cached mode: iterate class directories\n",
    "            for cls_name, cls_idx in CLASS_MAP.items():\n",
    "                cls_dir = os.path.join(cache_frames_root, cls_name)\n",
    "                if not os.path.isdir(cls_dir):\n",
    "                    continue\n",
    "                clip_dirs = [d for d in glob.glob(os.path.join(cls_dir, \"*\")) if os.path.isdir(d)]\n",
    "                for cd in clip_dirs:\n",
    "                    self.samples.append((cd, cls_idx, True))\n",
    "        # raw video mode\n",
    "        for ext in (\"*.mp4\",\"*.avi\",\"*.mov\",\"*.mkv\"):\n",
    "            for vp in glob.glob(os.path.join(video_root, ext)):\n",
    "                label = infer_label_from_filename(os.path.basename(vp))\n",
    "                if label is None:\n",
    "                    continue\n",
    "                self.samples.append((vp, label, False))\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(\"No video samples found.\")\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def _load_cached_frames(self, folder: str) -> List[Image.Image]:\n",
    "        frame_files = sorted(glob.glob(os.path.join(folder, \"frame_*.jpg\")))\n",
    "        imgs = []\n",
    "        for fp in frame_files[:self.max_seq_len*2]:  # allow extra\n",
    "            try:\n",
    "                imgs.append(Image.open(fp).convert(\"RGB\"))\n",
    "            except:\n",
    "                pass\n",
    "        return imgs\n",
    "    def __getitem__(self, idx: int):\n",
    "        path, label, is_cached = self.samples[idx]\n",
    "        if is_cached:\n",
    "            frames = self._load_cached_frames(path)\n",
    "        else:\n",
    "            frames = extract_frames_from_video(path, max_frames=self.max_seq_len*2)\n",
    "        if len(frames) == 0:\n",
    "            raise RuntimeError(f\"Failed to decode frames for {path}\")\n",
    "        # Choose prefix length\n",
    "        if self.training:\n",
    "            L = random.randint(self.min_prefix_len, min(self.max_seq_len, len(frames)))\n",
    "        else:\n",
    "            L = min(self.max_seq_len, len(frames))\n",
    "        selected = frames[:L]\n",
    "        # Pad if shorter than max_seq_len (consistent tensor length optional)\n",
    "        pad_needed = self.max_seq_len - L\n",
    "        if pad_needed > 0:\n",
    "            # repeat last frame\n",
    "            selected.extend([selected[-1]] * pad_needed)\n",
    "        tensor_frames = []\n",
    "        for img in selected[:self.max_seq_len]:\n",
    "            tensor_frames.append(self.transform(img))\n",
    "        seq = torch.stack(tensor_frames, dim=0)  # (T,C,H,W)\n",
    "        return seq, label\n",
    "\n",
    "# Example: building training / validation loaders (adjust paths)\n",
    "# video_train_root = \"videos/train\"\n",
    "# video_val_root   = \"videos/val\"\n",
    "# Optionally pre-extract frames:\n",
    "# preextract_video_frames(video_train_root, \"frames_cache/train\")\n",
    "# preextract_video_frames(video_val_root,   \"frames_cache/val\")\n",
    "\n",
    "def build_video_loaders(video_train_root: str,\n",
    "                        video_val_root: str,\n",
    "                        batch_size: int = 16,\n",
    "                        cache_train: Optional[str] = None,\n",
    "                        cache_val: Optional[str] = None):\n",
    "    train_ds = VideoEarlyGestureDataset(video_train_root, cache_frames_root=cache_train, training=True)\n",
    "    val_ds   = VideoEarlyGestureDataset(video_val_root,   cache_frames_root=cache_val, training=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# EARLY METRICS EVALUATION FOR VIDEO PREFIXES\n",
    "def evaluate_video_prefixes(model, loader, threshold: float = 0.6, device: torch.device = DEVICE):\n",
    "    model.eval()\n",
    "    correct_final = 0\n",
    "    correct_early = 0\n",
    "    confident_total = 0\n",
    "    correct_confident = 0\n",
    "    first_correct_ts = []\n",
    "    first_confident_correct_ts = []\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, label in loader:\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "            logits_time = model(seq)  # (B,T,C)\n",
    "            probs_time = torch.softmax(logits_time, dim=-1)\n",
    "            final_preds = probs_time[:, -1].argmax(dim=-1)\n",
    "            correct_final += (final_preds == label).sum().item()\n",
    "            B,T,_ = probs_time.shape\n",
    "            for b in range(B):\n",
    "                gt = label[b].item()\n",
    "                earliest_correct = None\n",
    "                earliest_confident_correct = None\n",
    "                confident_any = False\n",
    "                for t in range(T):\n",
    "                    p = probs_time[b,t]\n",
    "                    pred_t = int(p.argmax().item())\n",
    "                    conf_t = float(p.max().item())\n",
    "                    if pred_t == gt and earliest_correct is None:\n",
    "                        earliest_correct = t\n",
    "                    if conf_t >= threshold:\n",
    "                        confident_any = True\n",
    "                        if pred_t == gt and earliest_confident_correct is None:\n",
    "                            earliest_confident_correct = t\n",
    "                    if earliest_correct is not None and earliest_confident_correct is not None:\n",
    "                        break\n",
    "                if earliest_correct is not None:\n",
    "                    correct_early += 1\n",
    "                    first_correct_ts.append(earliest_correct+1)\n",
    "                if confident_any:\n",
    "                    confident_total += 1\n",
    "                if earliest_confident_correct is not None:\n",
    "                    correct_confident += 1\n",
    "                    first_confident_correct_ts.append(earliest_confident_correct+1)\n",
    "            total += B\n",
    "    final_acc = correct_final / max(1,total)\n",
    "    early_cover = correct_early / max(1,total)\n",
    "    confident_cover = confident_total / max(1,total)\n",
    "    confident_correct_rate = correct_confident / max(1,total)\n",
    "    avg_first_correct = sum(first_correct_ts)/len(first_correct_ts) if first_correct_ts else math.nan\n",
    "    avg_first_conf_correct = (sum(first_confident_correct_ts)/len(first_confident_correct_ts)\n",
    "                              if first_confident_correct_ts else math.nan)\n",
    "    return {\n",
    "        'final_acc': final_acc,\n",
    "        'early_cover': early_cover,\n",
    "        'confident_cover': confident_cover,\n",
    "        'confident_correct_rate': confident_correct_rate,\n",
    "        'avg_first_correct_t': avg_first_correct,\n",
    "        'avg_first_confident_correct_t': avg_first_conf_correct\n",
    "    }\n",
    "\n",
    "# REAL-TIME BUFFERED VIDEO INFERENCE (simulate with a video file)\n",
    "def stream_video_early(model, video_path: str, threshold: float = 0.6,\n",
    "                       max_buffer: int = MAX_SEQ_LEN, device: torch.device = DEVICE):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open:\", video_path)\n",
    "        return\n",
    "    buffer_frames: List[torch.Tensor] = []\n",
    "    model.eval()\n",
    "    frame_count = 0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil = Image.fromarray(rgb)\n",
    "            tensor = eval_video_transform(pil).unsqueeze(0)  # (1,C,H,W)\n",
    "            buffer_frames.append(tensor)\n",
    "            if len(buffer_frames) > max_buffer:\n",
    "                buffer_frames.pop(0)\n",
    "            frame_count += 1\n",
    "            if len(buffer_frames) >= MIN_PREFIX_LEN:\n",
    "                seq = torch.cat(buffer_frames, dim=0).unsqueeze(0).to(device)  # (1,T,C,H,W)\n",
    "                logits_time = model(seq)  # (1,T,C)\n",
    "                probs_time = torch.softmax(logits_time, dim=-1)[0]\n",
    "                # Check earliest confident\n",
    "                earliest_confident = None\n",
    "                for t in range(probs_time.size(0)):\n",
    "                    p = probs_time[t]\n",
    "                    conf = float(p.max().item())\n",
    "                    if conf >= threshold:\n",
    "                        earliest_confident = (t+1, int(p.argmax().item()), conf)\n",
    "                        break\n",
    "                if earliest_confident:\n",
    "                    print(f\"[frame {frame_count}] EARLY PRED timestep={earliest_confident[0]} \"\n",
    "                          f\"class={INV_CLASS_MAP[earliest_confident[1]]} conf={earliest_confident[2]:.2f}\")\n",
    "    cap.release()\n",
    "\n",
    "# EXAMPLE USAGE (uncomment and set paths):\n",
    "# train_loader, val_loader = build_video_loaders(\"videos/train\", \"videos/val\",\n",
    "#                                                batch_size=16,\n",
    "#                                                cache_train=\"frames_cache/train\",\n",
    "#                                                cache_val=\"frames_cache/val\")\n",
    "# metrics = evaluate_video_prefixes(model_early, val_loader)\n",
    "# print(metrics)\n",
    "# stream_video_early(model_early, \"videos/val/rock_clip23.mp4\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# NOTES SUMMARY (in-code):\n",
    "# - Keep full clips; generate prefixes dynamically for training early predictions.\n",
    "# - More variation (users/backgrounds) improves generalization.\n",
    "# - Data transfer to VM: compress, pre-extract frames, keep only needed frames.\n",
    "# - Extend MAX_SEQ_LEN if gestures need more temporal context; adjust weighting strategy.\n",
    "# - Consider adding a \"neutral\" class if initial frames truly have no gesture (requires annotation).\n",
    "# ----------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
