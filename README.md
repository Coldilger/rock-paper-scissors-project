# ZeroLag RPS: Real-Time Anticipation of Hand Gestures

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0-ee4c2c)
![MediaPipe](https://img.shields.io/badge/MediaPipe-Solutions-blueviolet)
![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-green)

> **Final Project for Deep Learning for Computer Vision (20600)**
> *Bocconi University, MSc in Data Science*

## ðŸ“œ Abstract
Real-time gesture recognition systems often suffer from latency due to the classification of completed motions. **ZeroLag RPS** is a model designed to defeat a human opponent in Rock-Paper-Scissors by predicting moves *before* gesture completion.

We developed a hybrid architecture combining a **ResNet-18 spatial encoder** with a **Temporal Convolutional Network (TCN) head**, trained on a custom dataset of gesture sequences. The TCN's dilated causal convolutions efficiently model temporal dependencies within a 64-frame window, enabling the detection of subtle "wind-up" micromovements. Deployed in a live environment, the system delivers zero-latency counter-moves for a seamless user experience.
## ðŸŽ¥ Demo
![System Demo](docs/rps_s.gif)

*The system predicts "Scissors" (and plays "Rock") while the hand is still in the wind-up phase.*

## ðŸ§  Model Architecture

Our approach solves the **"Rock Paradox"** (where all moves start looking like Rock) using a specialized pipeline:

1.  **Spatial Focus:** **MediaPipe Hands** extracts skeletal crops to ensure translation invariance and remove background noise.
2.  **Feature Extraction:** A **ResNet-18** encoder converts frames into feature vectors.
3.  **Temporal Modeling:** A **TCN (Temporal Convolutional Network)** processes a rolling buffer of 64 frames. We chose TCNs over LSTMs to enable parallelization and precise receptive field engineering.
4.  **Optimization:** Trained using a **Sigmoid Time-Weighted Loss** to penalize early-game ambiguity less than late-game precision.

## ðŸ“‚ Project Structure

```text
ZeroLag-RPS/
â”œâ”€â”€ data/                  # Dataset placeholder (download link below)
â”œâ”€â”€ docs/                  # Documentation, Report, and Slides
â”‚   â”œâ”€â”€ Project_CV_report.pdf
â”‚   â””â”€â”€ Presentation.pptx
â”œâ”€â”€ models/                # Trained model weights (.pth)
â”œâ”€â”€ notebooks/             # Experiments and drafts
â”‚   â”œâ”€â”€ TCN_training.ipynb
â”‚   â””â”€â”€ data_analysis.ipynb
â”œâ”€â”€ scripts/               # Source code
â”‚   â”œâ”€â”€ live_inference.py  # Main script for the live battle
â”‚   â”œâ”€â”€ preprocessing.py   # Hand crop and normalization logic
â”‚   â””â”€â”€ utils.py           # Helper functions
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

## ðŸš€ Installation & Usage

### 1. Clone the repository
```bash
git clone [https://github.com/Coldilger/ZeroLag-RPS.git](https://github.com/Coldilger/ZeroLag-RPS.git)
cd ZeroLag-RPS
```
### 2. Install dependencies
```bash
pip install -r requirements.txt
```
### 3. Run the Live Agent
```bash
python scripts/live_inference.py
```

## ðŸ“Š Dataset
The model was trained on a custom dataset comprising ~5,300 frames of Rock, Paper, and Scissors gestures recorded at 20fps.
- Download the dataset [here](https://drive.google.com/drive/folders/1yZMYdbUkMhnvQazs3OVpYUC7R1Ky9l4b?usp=sharing)

## ðŸ‘¥ Contributors
- Ilia Koldyshev
- Gaia Iori
- Parsa Bakhtiari
